% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_fma_thompson_sampling.R
\name{ThompsonSamplingPolicy}
\alias{ThompsonSamplingPolicy}
\title{R6 Class representing the Finitely Many Arm Thompson Sampling policy}
\description{
A \code{\link{ThompsonSamplingPolicy}} object implements the
  behaviour of an agent with a Thompson Sampling Policy. It is instantiated
  without arguments. The method \code{\link{run}} processes a reward matrix
  corresponding to a binary stochastic with finitely many arms bandit.
}
\details{
The Thompson Sampling Algorithm is used in binary stochastic bandits
  with finitely many arms problems.

  When processing a reward matrix, a reward distribution is attributed to
  each arm and then sampled randomly at each round. Alpha and beta attributes
  stores those distributions parameters (Not to be confused with the UCB
  parameter alpha !). They are reset at the beginning of the simulation then
  updated depending on the observed reward.

  The beauty of Thompson Sampling is that prior and posterior distribution
  are both beta distributions when rewards are binary, simplifying the
  computations.

  This class uses a \code{\link{Recorder}} object to export results.
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{name}}{Short string describing the instance.}

\item{\code{record}}{R6 object of class Recorder used to export simulation
results.}

\item{\code{K}}{Integer containing the number of arms.}

\item{\code{beta}}{Numeric vector containing the beta parameter of each arm's
distribution.}

\item{\code{alpha}}{Numeric vector containing the alpha parameter of each arm's
distribution.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{ThompsonSamplingPolicy$new()}}
\item \href{#method-reset}{\code{ThompsonSamplingPolicy$reset()}}
\item \href{#method-get_sample_of_an_arm}{\code{ThompsonSamplingPolicy$get_sample_of_an_arm()}}
\item \href{#method-get_sample_of_all_arms}{\code{ThompsonSamplingPolicy$get_sample_of_all_arms()}}
\item \href{#method-get_mean_of_an_arm}{\code{ThompsonSamplingPolicy$get_mean_of_an_arm()}}
\item \href{#method-get_mean_of_all_arms}{\code{ThompsonSamplingPolicy$get_mean_of_all_arms()}}
\item \href{#method-get_action}{\code{ThompsonSamplingPolicy$get_action()}}
\item \href{#method-update_with_reward}{\code{ThompsonSamplingPolicy$update_with_reward()}}
\item \href{#method-run}{\code{ThompsonSamplingPolicy$run()}}
\item \href{#method-clone}{\code{ThompsonSamplingPolicy$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Creates a new ThompsonSamplingPolicy object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$new()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A new `ThompsonSamplingObject` object.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-reset"></a>}}
\if{latex}{\out{\hypertarget{method-reset}{}}}
\subsection{Method \code{reset()}}{
Set or reset object attributes before starting a simulation.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$reset(K)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{K}}{Number of arms.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_sample_of_an_arm"></a>}}
\if{latex}{\out{\hypertarget{method-get_sample_of_an_arm}{}}}
\subsection{Method \code{get_sample_of_an_arm()}}{
Samples an arm reward distributions.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$get_sample_of_an_arm(a)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{a}}{the arm index.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A number between 0 and 1.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_sample_of_all_arms"></a>}}
\if{latex}{\out{\hypertarget{method-get_sample_of_all_arms}{}}}
\subsection{Method \code{get_sample_of_all_arms()}}{
Samples all arm reward distributions.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$get_sample_of_all_arms()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A numeric vector containing each arm sample.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_mean_of_an_arm"></a>}}
\if{latex}{\out{\hypertarget{method-get_mean_of_an_arm}{}}}
\subsection{Method \code{get_mean_of_an_arm()}}{
Computes an arm reward expectation according to its
  distribution.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$get_mean_of_an_arm(a)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{a}}{the arm index.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
The arm reward expectation.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_mean_of_all_arms"></a>}}
\if{latex}{\out{\hypertarget{method-get_mean_of_all_arms}{}}}
\subsection{Method \code{get_mean_of_all_arms()}}{
Computes all arm reward expectation according to their
  distributions.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$get_mean_of_all_arms()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
A numeric vector containing each arm reward expectation.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_action"></a>}}
\if{latex}{\out{\hypertarget{method-get_action}{}}}
\subsection{Method \code{get_action()}}{
Chooses which arm to pull by sampling the arms reward
  distribution then taking the max.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$get_action()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
The index of the chosen arm.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-update_with_reward"></a>}}
\if{latex}{\out{\hypertarget{method-update_with_reward}{}}}
\subsection{Method \code{update_with_reward()}}{
Updates alpha and beta given reward r of arm a.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$update_with_reward(r, a)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{r}}{Reward.}

\item{\code{a}}{Arm.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-run"></a>}}
\if{latex}{\out{\hypertarget{method-run}{}}}
\subsection{Method \code{run()}}{
Process a bandit problem represented by the reward matrix.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$run(rewards, verbose = TRUE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{rewards}}{Reward matrix. Time on rows, arms on columns.}

\item{\code{verbose}}{Logical. if FALSE, only action and reward history will be
returned. Default is TRUE.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A tibble describing at each iteration, the means of each arm,
  the action taken and the reward received. If verbose is FALSE, only the
  action and reward.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{ThompsonSamplingPolicy$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
