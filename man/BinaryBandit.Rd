% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bandit_binary_fma.R
\name{BinaryBandit}
\alias{BinaryBandit}
\title{R6 Class handling a set of binary stochastic arms and compatible policies}
\description{
A BinaryBandit object holds a set of of binary arms represented
  by their mean and generated reward matrix. It provides methods to run
  policies against these arms.
}
\details{
A BinaryBandit Object is instantiated using the horizon and the
  means of each arms.  Additional arms can be provided afterward using
  \code{\link{add_bernouilli_arm}}.

  Policies can be simulated against the arms using
  \code{\link{run_ucb_policy}} or \code{\link{run_ts_policy}}.

  Plot of cumulative regret can be generated using \code{\link{plot_regret}}
}
\examples{
bandit <- BinaryBandit$new(horizon=1000L, arms_means=c(0.2,0.5))
ucbres <- bandit$run_ucb_policy(alpha=c(1,10),verbose=TRUE)
tsres <- bandit$run_ts_policy(verbose=TRUE)
bandit$plot_regret(ucb_res[[1]]$result)
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{arms_means}}{Numeric vector containing each arm's mean.}

\item{\code{horizon}}{Integer specifying the number of time steps to run.}

\item{\code{rewards}}{Matrix containing arms rewards. Time step on rows, arms on
columns.}

\item{\code{K}}{Number of arms.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{BinaryBandit$new()}}
\item \href{#method-add_bernouilli_arm}{\code{BinaryBandit$add_bernouilli_arm()}}
\item \href{#method-reset_rewards}{\code{BinaryBandit$reset_rewards()}}
\item \href{#method-run_ucb_policy}{\code{BinaryBandit$run_ucb_policy()}}
\item \href{#method-run_ts_policy}{\code{BinaryBandit$run_ts_policy()}}
\item \href{#method-plot_regret}{\code{BinaryBandit$plot_regret()}}
\item \href{#method-clone}{\code{BinaryBandit$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Create a new BinaryBandit object.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$new(horizon, arms_means)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{horizon}}{Number of time steps. Determines height of reward matrix.}

\item{\code{arms_means}}{Vector specifying the arms means.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new `Binary Bandit` object.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-add_bernouilli_arm"></a>}}
\if{latex}{\out{\hypertarget{method-add_bernouilli_arm}{}}}
\subsection{Method \code{add_bernouilli_arm()}}{
Adds an arm.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$add_bernouilli_arm(p)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{p}}{Reward expectation of the arm.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-reset_rewards"></a>}}
\if{latex}{\out{\hypertarget{method-reset_rewards}{}}}
\subsection{Method \code{reset_rewards()}}{
Re-generates the reward matrix using existing arms means.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$reset_rewards()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-run_ucb_policy"></a>}}
\if{latex}{\out{\hypertarget{method-run_ucb_policy}{}}}
\subsection{Method \code{run_ucb_policy()}}{
Runs a set of UCB policies against the arms.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$run_ucb_policy(alpha, verbose = TRUE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{alpha}}{Numeric vector specifying UCB policies parameters.}

\item{\code{verbose}}{Boolean specifying the verbose mode of simulation.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A list of simulation result. An item of this list consist of a
  policy name, the result of the simulation of that policy and the system
  time the simulation took.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-run_ts_policy"></a>}}
\if{latex}{\out{\hypertarget{method-run_ts_policy}{}}}
\subsection{Method \code{run_ts_policy()}}{
Runs a Thompson Sampling policy against the arms.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$run_ts_policy(verbose = TRUE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{verbose}}{Boolean specifying the verbose mode of simulation.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A list containing the policy name, the result of the simulation
  and the system time the simulation took.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-plot_regret"></a>}}
\if{latex}{\out{\hypertarget{method-plot_regret}{}}}
\subsection{Method \code{plot_regret()}}{
Plots the cumulative regret of a result.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$plot_regret(result)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{result}}{A simulation as returned by a policy, i.e. the result field
of a run_ucb_policy output item.}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A plot with as its x axis the time steps and as its y axis the
  cumulative regret.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{BinaryBandit$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
